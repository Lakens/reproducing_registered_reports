@article{ragan2018binder,
  title={Binder 2.0-Reproducible, interactive, sharable environments for science at scale},
  author={Ragan-Kelley, Benjamin and Willing, Carol},
  year={2018}
}

@article{brinckman_computing_2019,
	title = {Computing environments for reproducibility: {Capturing} the “{Whole} {Tale}”},
	volume = {94},
	issn = {0167-739X},
	shorttitle = {Computing environments for reproducibility},
	url = {http://www.sciencedirect.com/science/article/pii/S0167739X17310695},
	doi = {10.1016/j.future.2017.12.029},
	abstract = {The act of sharing scientific knowledge is rapidly evolving away from traditional articles and presentations to the delivery of executable objects that integrate the data and computational details (e.g., scripts and workflows) upon which the findings rely. This envisioned coupling of data and process is essential to advancing science but faces technical and institutional barriers. The Whole Tale project aims to address these barriers by connecting computational, data-intensive research efforts with the larger research process—transforming the knowledge discovery and dissemination process into one where data products are united with research articles to create “living publications” or tales. The Whole Tale focuses on the full spectrum of science, empowering users in the long tail of science, and power users with demands for access to big data and compute resources. We report here on the design, architecture, and implementation of the Whole Tale environment.},
	language = {en},
	urldate = {2020-01-12},
	journal = {Future Generation Computer Systems},
	author = {Brinckman, Adam and Chard, Kyle and Gaffney, Niall and Hategan, Mihael and Jones, Matthew B. and Kowalik, Kacper and Kulasekaran, Sivakumar and Ludäscher, Bertram and Mecum, Bryce D. and Nabrzyski, Jarek and Stodden, Victoria and Taylor, Ian J. and Turk, Matthew J. and Turner, Kandace},
	month = may,
	year = {2019},
	keywords = {Code sharing, Data sharing, Living publications, Provenance, Reproducibility},
	pages = {854--867},
	file = {Brinckman et al. - 2019 - Computing environments for reproducibility Captur.pdf:C\:\\Users\\Daniel\\Zotero\\storage\\C7YAA49G\\Brinckman et al. - 2019 - Computing environments for reproducibility Captur.pdf:application/pdf;Computing environments for reproducibility\: Capturing the “Whole Tale”:C\:\\Users\\Daniel\\Zotero\\storage\\8NU862QL\\brinckman2018.html:text/html;ScienceDirect Snapshot:C\:\\Users\\Daniel\\Zotero\\storage\\RA6XCCFE\\S0167739X17310695.html:text/html}
}

@article{campbell_self-esteem_2018,
  series = {Replication of {{Critical Findings}} in {{Personality Psychology}}},
  title = {Self-Esteem, Relationship Threat, and Dependency Regulation: {{Independent}} Replication of {{Murray}}, {{Rose}}, {{Bellavia}}, {{Holmes}}, and {{Kusche}} (2002) {{Study}} 3},
  volume = {72},
  issn = {0092-6566},
  shorttitle = {Self-Esteem, Relationship Threat, and Dependency Regulation},
  abstract = {Across three studies, Murray, Rose, Bellavia, Holmes, and Kusche (2002) found that low self-esteem individuals responded in a negative manner compared to those high in self-esteem in the face of relationship threat, perceiving their partners and relationships less positively. This was the first empirical support for the hypothesized dynamics of a dependency regulation perspective, and has had a significant impact on the field of relationship science. In the present research, we sought to reproduce the methods and procedures of Study 3 of Murray et al. (2002) to further test the two-way interaction between individual differences in self-esteem and situational relationship threat. Manipulation check effects replicated the original study, but no interaction between self-esteem and experimental condition was observed for any primary study outcomes.},
  journal = {Journal of Research in Personality},
  doi = {10.1016/j.jrp.2017.04.001},
  author = {Campbell, Lorne and Balzarini, Rhonda N. and Kohut, Taylor and Dobson, Kiersten and Hahn, Christian M. and Moroz, Sarah E. and Stanton, Sarah C. E.},
  month = feb,
  year = {2018},
  keywords = {Reproducibility,Rejection,Replication,Self-esteem,Romantic relationships},
  pages = {5-9}
}

@article{chambers_instead_2014,
  title = {Instead of "Playing the Game" It Is Time to Change the Rules: {{Registered Reports}} at {{AIMS Neuroscience}} and Beyond},
  volume = {1},
  copyright = {cc\_by},
  shorttitle = {Instead of "Playing the Game" It Is Time to Change the Rules},
  abstract = {The last ten years have witnessed increasing awareness of questionable research practices (QRPs) in the life sciences [1,2], including p-hacking [3], HARKing [4], lack of replication [5], publication bias [6], low statistical power [7] and lack of data sharing ([8]; see Figure 1). Concerns about such behaviours have been raised repeatedly for over half a century [9\textendash{}11] but the incentive structure of academia has not changed to address them.
Despite the complex motivations that drive academia, many QRPs stem from the simple fact that the incentives which offer success to individual scientists conflict with what is best for science [12]. On the one hand are a set of gold standards that centuries of the scientific method have proven to be crucial for discovery: rigour, reproducibility, and transparency. On the other hand are a set of opposing principles born out of the academic career model: the drive to produce novel and striking results, the importance of confirming prior expectations, and the need to protect research interests from competitors. Within a culture that pressures scientists to produce rather than discover, the outcome is a biased and impoverished science in which most published results are either unconfirmed genuine discoveries or unchallenged fallacies [13]. This observation implies no moral judgement of scientists, who are as much victims of this system as they are perpetrators.},
  language = {en},
  journal = {AIMS Neuroscience},
  doi = {10.3934/Neuroscience2014.1.4},
  author = {Chambers, Christopher D. and Feredoes, Eva and Muthukumaraswamy, Suresh Daniel and Etchells, Peter},
  month = may,
  year = {2014},
  pages = {4-17}
}

@article{gertler_reference_2017,
  title = {Reference {{Rot}}: {{An Emerging Threat}} to {{Transparency}} in {{Political Science}}},
  volume = {50},
  issn = {1049-0965, 1537-5935},
  shorttitle = {Reference {{Rot}}},
  abstract = {Transparency of research is a large concern in political science, and the practice of publishing links to datasets and other online resources is one of the main methods by which political scientists promote transparency. But the method cannot work if the links don't, and very often, they don't. We show that most of the URLs ever published in the American Political Science Review no longer work as intended. The problem is severe in recent as well as in older articles; for example, more than one-fourth of links published in the APSR in 2013 were broken by the end of 2014. We conclude that ``reference rot'' limits the transparency and reproducibility of political science research. We also describe practices that scholars can adopt to combat the problem: when possible, they should archive data in trustworthy repositories, use links that incorporate persistent digital identifiers, and create archival versions of the webpages to which they link.},
  language = {en},
  number = {1},
  journal = {PS: Political Science \& Politics},
  doi = {10.1017/S1049096516002353},
  author = {Gertler, Aaron L. and Bullock, John G.},
  month = jan,
  year = {2017},
  pages = {166-171}
}

@article{clyburne-sherin_computational_2018,
  title = {Computational {{Reproducibility}} via {{Containers}} in {{Social Psychology}}},
  abstract = {NOTE: Revised version currently under submission at Met-psychology. Under peer review at Meta-Psychology, submission number MP2018.892, link: https://osf.io/ps5ru/. Anyone can participate in peer review by sending the editor an email, or through discussion on social media. The preferred way of open commenting, however, is to use the hypothes.is integration at PsyArXiv and directly comment on this preprint. 

Editor: Rickard Carlsson, rickard.carlsson@lnu.se

Website: https://open.lnu.se/index.php/metapsychology 
ABSTRACT: Scientific progress relies on the replication and reuse of research. However, despite an emerging culture of sharing code and data in psychology, the research practices needed to achieve computational reproducibility -- the quality of a research project entailing the provision of sufficient code, data and documentation to allow an independent researcher to re-obtain the project's results -- are not widely adopted. Historically, the ability to share and reuse computationally reproducible research was technically challenging and time-consuming. One welcome development on this front is the advent of containers, a technology intended to facilitate code sharing for software development. Containers, however, remain technically demanding and imperfectly suited for research applications. This editorial argues that the use of containers adapted for research can help foster a culture of reproducibiliy in psychology research. We will illustrate this by introducing Code Ocean, an online computational reproducibility platform. (Disclaimer: the authors work for Code Ocean.)},
  doi = {10.31234/osf.io/mf82t},
  author = {{Clyburne-Sherin}, April and Fei, Xu and Green, Seth Ariel},
  month = feb,
  year = {2018}
}

@article{hagger_multilab_2016,
  title = {A {{Multilab Preregistered Replication}} of the {{Ego}}-{{Depletion Effect}}},
  volume = {11},
  issn = {1745-6916},
  abstract = {Good self-control has been linked to adaptive outcomes such as better health, cohesive personal relationships, success in the workplace and at school, and less susceptibility to crime and addictions. In contrast, self-control failure is linked to maladaptive outcomes. Understanding the mechanisms by which self-control predicts behavior may assist in promoting better regulation and outcomes. A popular approach to understanding self-control is the strength or resource depletion model. Self-control is conceptualized as a limited resource that becomes depleted after a period of exertion resulting in self-control failure. The model has typically been tested using a sequential-task experimental paradigm, in which people completing an initial self-control task have reduced self-control capacity and poorer performance on a subsequent task, a state known as ego depletion. Although a meta-analysis of ego-depletion experiments found a medium-sized effect, subsequent meta-analyses have questioned the size and existence of the effect and identified instances of possible bias. The analyses served as a catalyst for the current Registered Replication Report of the ego-depletion effect. Multiple laboratories (k = 23, total N = 2,141) conducted replications of a standardized ego-depletion protocol based on a sequential-task paradigm by Sripada et al. Meta-analysis of the studies revealed that the size of the ego-depletion effect was small with 95\% confidence intervals (CIs) that encompassed zero (d = 0.04, 95\% CI [-0.07, 0.15]. We discuss implications of the findings for the ego-depletion effect and the resource depletion model of self-control.},
  language = {en},
  number = {4},
  journal = {Perspectives on Psychological Science},
  doi = {10.1177/1745691616652873},
  author = {Hagger, M. S. and Chatzisarantis, N. L. D. and Alberts, H. and Anggono, C. O. and Batailler, C. and Birt, A. R. and Brand, R. and Brandt, M. J. and Brewer, G. and Bruyneel, S. and Calvillo, D. P. and Campbell, W. K. and Cannon, P. R. and Carlucci, M. and Carruth, N. P. and Cheung, T. and Crowell, A. and De Ridder, D. T. D. and Dewitte, S. and Elson, M. and Evans, J. R. and Fay, B. A. and Fennis, B. M. and Finley, A. and Francis, Z. and Heise, E. and Hoemann, H. and Inzlicht, M. and Koole, S. L. and Koppel, L. and Kroese, F. and Lange, F. and Lau, K. and Lynch, B. P. and Martijn, C. and Merckelbach, H. and Mills, N. V. and Michirev, A. and Miyake, A. and Mosser, A. E. and Muise, M. and Muller, D. and Muzi, M. and Nalis, D. and Nurwanti, R. and Otgaar, H. and Philipp, M. C. and Primoceri, P. and Rentzsch, K. and Ringos, L. and Schlinkert, C. and Schmeichel, B. J. and Schoch, S. F. and Schrama, M. and Sch{\"u}tz, A. and Stamos, A. and Tingh{\"o}g, G. and Ullrich, J. and {vanDellen}, M. and Wimbarti, S. and Wolff, W. and Yusainy, C. and Zerhouni, O. and Zwienenberg, M.},
  month = jul,
  year = {2016},
  pages = {546-573}
}

@article{klein_practical_2018,
  title = {A {{Practical Guide}} for {{Transparency}} in {{Psychological Science}}},
  volume = {4},
  copyright = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are \textcopyright, \textregistered{} or \texttrademark{} of their respective owners. No challenge to any owner's rights is intended or should be inferred.},
  issn = {2474-7394},
  abstract = {Article: A Practical Guide for Transparency in Psychological Science},
  language = {eng},
  number = {1},
  journal = {Collabra: Psychology},
  doi = {10.1525/collabra.158},
  author = {Klein, Olivier and Hardwicke, Tom E. and Aust, Frederik and Breuer, Johannes and Danielsson, Henrik and Mohr, Alicia Hofelich and Ijzerman, Hans and Nilsonne, Gustav and Vanpaemel, Wolf and Frank, Michael C.},
  month = jun,
  year = {2018},
  pages = {20}
}

@article{klein_investigating_2014,
  title = {Investigating {{Variation}} in {{Replicability}}},
  volume = {45},
  issn = {1864-9335},
  abstract = {Although replication is a central tenet of science, direct replications are rare in psychology. This research tested variation in the replicability of 13 classic and contemporary effects across 36 independent samples totaling 6,344 participants. In the aggregate, 10 effects replicated consistently. One effect \textendash{} imagined contact reducing prejudice \textendash{} showed weak support for replicability. And two effects \textendash{} flag priming influencing conservatism and currency priming influencing system justification \textendash{} did not replicate. We compared whether the conditions such as lab versus online or US versus international sample predicted effect magnitudes. By and large they did not. The results of this small sample of effects suggest that replicability is more dependent on the effect itself than on the sample and setting used to investigate the effect.},
  number = {3},
  journal = {Social Psychology},
  doi = {10.1027/1864-9335/a000178},
  author = {Klein, Richard A. and Ratliff, Kate A. and Vianello, Michelangelo and Adams, Reginald B. and Bahn{\'i}k, {\v S}t{\v e}p{\'a}n and Bernstein, Michael J. and Bocian, Konrad and Brandt, Mark J. and Brooks, Beach and Brumbaugh, Claudia Chloe and Cemalcilar, Zeynep and Chandler, Jesse and Cheong, Winnee and Davis, William E. and Devos, Thierry and Eisner, Matthew and Frankowska, Natalia and Furrow, David and Galliani, Elisa Maria and Hasselman, Fred and Hicks, Joshua A. and Hovermale, James F. and Hunt, S. Jane and Huntsinger, Jeffrey R. and IJzerman, Hans and John, Melissa-Sue and {Joy-Gaba}, Jennifer A. and Barry Kappes, Heather and Krueger, Lacy E. and Kurtz, Jaime and Levitan, Carmel A. and Mallett, Robyn K. and Morris, Wendy L. and Nelson, Anthony J. and Nier, Jason A. and Packard, Grant and Pilati, Ronaldo and Rutchick, Abraham M. and Schmidt, Kathleen and Skorinko, Jeanine L. and Smith, Robert and Steiner, Troy G. and Storbeck, Justin and Van Swol, Lyn M. and Thompson, Donna and {van `t Veer}, A. E. and Ann Vaughn, Leigh and Vranka, Marek and Wichman, Aaron L. and Woodzicka, Julie A. and Nosek, Brian A.},
  month = jan,
  year = {2014},
  pages = {142-152}
}

@article{klein_many_2018,
  title = {Many {{Labs}} 2: {{Investigating Variation}} in {{Replicability Across Samples}} and {{Settings}}},
  volume = {1},
  issn = {2515-2459},
  shorttitle = {Many {{Labs}} 2},
  abstract = {We conducted preregistered replications of 28 classic and contemporary published findings, with protocols that were peer reviewed in advance, to examine variation in effect magnitudes across samples and settings. Each protocol was administered to approximately half of 125 samples that comprised 15,305 participants from 36 countries and territories. Using the conventional criterion of statistical significance (p {$<$} .05), we found that 15 (54\%) of the replications provided evidence of a statistically significant effect in the same direction as the original finding. With a strict significance criterion (p {$<$} .0001), 14 (50\%) of the replications still provided such evidence, a reflection of the extremely high-powered design. Seven (25\%) of the replications yielded effect sizes larger than the original ones, and 21 (75\%) yielded effect sizes smaller than the original ones. The median comparable Cohen's ds were 0.60 for the original findings and 0.15 for the replications. The effect sizes were small ({$<$} 0.20) in 16 of the replications (57\%), and 9 effects (32\%) were in the direction opposite the direction of the original effect. Across settings, the Q statistic indicated significant heterogeneity in 11 (39\%) of the replication effects, and most of those were among the findings with the largest overall effect sizes; only 1 effect that was near zero in the aggregate showed significant heterogeneity according to this measure. Only 1 effect had a tau value greater than .20, an indication of moderate heterogeneity. Eight others had tau values near or slightly above .10, an indication of slight heterogeneity. Moderation tests indicated that very little heterogeneity was attributable to the order in which the tasks were performed or whether the tasks were administered in lab versus online. Exploratory comparisons revealed little heterogeneity between Western, educated, industrialized, rich, and democratic (WEIRD) cultures and less WEIRD cultures (i.e., cultures with relatively high and low WEIRDness scores, respectively). Cumulatively, variability in the observed effect sizes was attributable more to the effect being studied than to the sample or setting in which it was studied.},
  language = {en},
  number = {4},
  journal = {Advances in Methods and Practices in Psychological Science},
  doi = {10.1177/2515245918810225},
  author = {Klein, Richard A. and Vianello, Michelangelo and Hasselman, Fred and Adams, Byron G. and Adams, Reginald B. and Alper, Sinan and Aveyard, Mark and Axt, Jordan R. and Babalola, Mayowa T. and Bahn{\'i}k, {\v S}t{\v e}p{\'a}n and Batra, Rishtee and Berkics, Mih{\'a}ly and Bernstein, Michael J. and Berry, Daniel R. and Bialobrzeska, Olga and Binan, Evans Dami and Bocian, Konrad and Brandt, Mark J. and Busching, Robert and R{\'e}dei, Anna Cabak and Cai, Huajian and Cambier, Fanny and Cantarero, Katarzyna and Carmichael, Cheryl L. and Ceric, Francisco and Chandler, Jesse and Chang, Jen-Ho and Chatard, Armand and Chen, Eva E. and Cheong, Winnee and Cicero, David C. and Coen, Sharon and Coleman, Jennifer A. and Collisson, Brian and Conway, Morgan A. and Corker, Katherine S. and Curran, Paul G. and Cushman, Fiery and Dagona, Zubairu K. and Dalgar, Ilker and Dalla Rosa, Anna and Davis, William E. and {de Bruijn}, Maaike and De Schutter, Leander and Devos, Thierry and {de Vries}, Marieke and Do{\u g}ulu, Canay and Dozo, Nerisa and Dukes, Kristin Nicole and Dunham, Yarrow and Durrheim, Kevin and Ebersole, Charles R. and Edlund, John E. and Eller, Anja and English, Alexander Scott and Finck, Carolyn and Frankowska, Natalia and Freyre, Miguel-{\'A}ngel and Friedman, Mike and Galliani, Elisa Maria and Gandi, Joshua C. and Ghoshal, Tanuka and Giessner, Steffen R. and Gill, Tripat and Gnambs, Timo and G{\'o}mez, {\'A}ngel and Gonz{\'a}lez, Roberto and Graham, Jesse and Grahe, Jon E. and Grahek, Ivan and Green, Eva G. T. and Hai, Kakul and Haigh, Matthew and Haines, Elizabeth L. and Hall, Michael P. and Heffernan, Marie E. and Hicks, Joshua A. and Houdek, Petr and Huntsinger, Jeffrey R. and Huynh, Ho Phi and IJzerman, Hans and Inbar, Yoel and {Innes-Ker}, {\AA}se H. and {Jim{\'e}nez-Leal}, William and John, Melissa-Sue and {Joy-Gaba}, Jennifer A. and Kamilo{\u g}lu, Roza G. and Kappes, Heather Barry and Karabati, Serdar and Karick, Haruna and Keller, Victor N. and Kende, Anna and Kervyn, Nicolas and Kne{\v z}evi{\'c}, Goran and Kovacs, Carrie and Krueger, Lacy E. and Kurapov, German and Kurtz, Jamie and Lakens, Dani{\"e}l and Lazarevi{\'c}, Ljiljana B. and Levitan, Carmel A. and Lewis, Neil A. and Lins, Samuel and Lipsey, Nikolette P. and Losee, Joy E. and Maassen, Esther and Maitner, Angela T. and Malingumu, Winfrida and Mallett, Robyn K. and Marotta, Satia A. and Me{\dj}edovi{\'c}, Janko and {Mena-Pacheco}, Fernando and Milfont, Taciano L. and Morris, Wendy L. and Murphy, Sean C. and Myachykov, Andriy and Neave, Nick and Neijenhuijs, Koen and Nelson, Anthony J. and Neto, F{\'e}lix and Lee Nichols, Austin and Ocampo, Aaron and O'Donnell, Susan L. and Oikawa, Haruka and Oikawa, Masanori and Ong, Elsie and Orosz, G{\'a}bor and Osowiecka, Malgorzata and Packard, Grant and {P{\'e}rez-S{\'a}nchez}, Rolando and Petrovi{\'c}, Boban and Pilati, Ronaldo and Pinter, Brad and Podesta, Lysandra and Pogge, Gabrielle and Pollmann, Monique M. H. and Rutchick, Abraham M. and Saavedra, Patricio and Saeri, Alexander K. and Salomon, Erika and Schmidt, Kathleen and Sch{\"o}nbrodt, Felix D. and Sekerdej, Maciej B. and Sirlop{\'u}, David and Skorinko, Jeanine L. M. and Smith, Michael A. and {Smith-Castro}, Vanessa and Smolders, Karin C. H. J. and Sobkow, Agata and Sowden, Walter and Spachtholz, Philipp and Srivastava, Manini and Steiner, Troy G. and Stouten, Jeroen and Street, Chris N. H. and Sundfelt, Oskar K. and Szeto, Stephanie and Szumowska, Ewa and Tang, Andrew C. W. and Tanzer, Norbert and Tear, Morgan J. and Theriault, Jordan and Thomae, Manuela and Torres, David and Traczyk, Jakub and Tybur, Joshua M. and Ujhelyi, Adrienn and {van Aert}, Robbie C. M. and {van Assen}, Marcel A. L. M. and {van der Hulst}, Marije and {van Lange}, Paul A. M. and {van 't Veer}, Anna Elisabeth and {V{\'a}squez- Echeverr{\'i}a}, Alejandro and Ann Vaughn, Leigh and V{\'a}zquez, Alexandra and Vega, Luis Diego and Verniers, Catherine and Verschoor, Mark and Voermans, Ingrid P. J. and Vranka, Marek A. and Welch, Cheryl and Wichman, Aaron L. and Williams, Lisa A. and Wood, Michael and Woodzicka, Julie A. and Wronska, Marta K. and Young, Liane and Zelenski, John M. and Zhijia, Zeng and Nosek, Brian A.},
  month = dec,
  year = {2018},
  pages = {443-490}
}

@misc{le_open_2018,
  title = {Open {{Science Manual}}},
  howpublished = {https://bit.ly/2w2F6Xu},
  author = {Le, Benjamin},
  month = may,
  year = {2018}
}

@article{nosek_registered_2014,
  title = {Registered {{Reports}}},
  volume = {45},
  issn = {1864-9335},
  number = {3},
  journal = {Social Psychology},
  doi = {10.1027/1864-9335/a000192},
  author = {Nosek, Brian A. and Lakens, Dani{\"e}l},
  month = jan,
  year = {2014},
  pages = {137-141}
}

@article{sandve_ten_2013,
  title = {Ten {{Simple Rules}} for {{Reproducible Computational Research}}},
  volume = {9},
  issn = {1553-7358},
  language = {en},
  number = {10},
  journal = {PLOS Computational Biology},
  doi = {10.1371/journal.pcbi.1003285},
  author = {Sandve, Geir Kjetil and Nekrutenko, Anton and Taylor, James and Hovig, Eivind},
  month = oct,
  year = {2013},
  keywords = {Reproducibility,Replication studies,Archives,Habits,Source code,Computer and information sciences,Sequence analysis,Computer applications},
  pages = {e1003285}
}

@article{stockemer_data_2018,
  title = {Data {{Access}}, {{Transparency}}, and {{Replication}}: {{New Insights}} from the {{Political Behavior Literature}}},
  volume = {51},
  issn = {1049-0965, 1537-5935},
  shorttitle = {Data {{Access}}, {{Transparency}}, and {{Replication}}},
  abstract = {Do researchers share their quantitative data and are the quantitative results that are published in political science journals replicable? We attempt to answer these questions by analyzing all articles published in the 2015 issues of three political behaviorist journals (i.e., Electoral Studies, Party Politics, and Journal of Elections, Public Opinion \& Parties)\textemdash{}all of which did not have a binding data-sharing and replication policy as of 2015. We found that authors are still reluctant to share their data; only slightly more than half of the authors in these journals do so. For those who share their data, we mainly confirmed the initial results reported in the respective articles in roughly 70\% of the times. Only roughly 5\% of the articles yielded significantly different results from those reported in the publication. However, we also found that roughly 25\% of the articles organized the data and/or code so poorly that replication was impossible.},
  language = {en},
  number = {4},
  journal = {PS: Political Science \& Politics},
  doi = {10.1017/S1049096518000926},
  author = {Stockemer, Daniel and Koehler, Sebastian and Lentz, Tobias},
  month = oct,
  year = {2018},
  pages = {799-803}
}

@article{voorspoels_can_2014,
  title = {Can Race Really Be Erased? {{A}} Pre-Registered Replication Study},
  volume = {5},
  issn = {1664-1078},
  shorttitle = {Can Race Really Be Erased?},
  abstract = {When encountering an unknown individual, social categorization of the individual has been shown to automatically proceed on the basis of three fundamental dimensions: People seem to mandatorily encode race, sex and age. In contradiction to this general finding, \textbackslash{}citeA\{Kurzbanetal2001\} showed that race encoding is not automatic and inevitable, but rather a byproduct of categorization in terms of coalitions. In particular, they argue and empirically support that when other coalitional information is present, the encoding of race is spectacularly reduced. In the present contribution, we present a replication of the race-erased effect reported by Kurzban et. al. First, we give a detailed overview of the hypotheses, the experimental methodology, the derivation of the sample size required to achieve a power of 95\textbackslash\%, and the criteria that need to be met for a successful replication. Then we present the findings of an empirical test that met the requirements of our power analyses. Our results indicate that the encoding of race can indeed be reduced when another coalitional cue is available, yet this reduction is less marked than in the original study. This experiment was preregistered before data collection at Open Science Framework, osf.io/vnhrm/.},
  language = {English},
  journal = {Frontiers in Psychology},
  doi = {10.3389/fpsyg.2014.01035},
  author = {Voorspoels, Wouter and Bartlema, Annelies and Vanpaemel, Wolf},
  year = {2014},
  keywords = {cognitive processing,coalitional psychology,Replication,Categorization,Social categorization}
}

@article{wesselmann_revisiting_2014,
  title = {Revisiting {{Schachter}}'s {{Research}} on {{Rejection}}, {{Deviance}}, and {{Communication}} (1951)},
  volume = {45},
  issn = {1864-9335},
  abstract = {We conducted a replication of the original Schachter (1951) deviation-rejection study. Schachter's groundbreaking demonstration of the deviation-rejection link has captivated social psychologists for decades. The findings and paradigm were so compelling that the deviation-rejection link is often taken for granted and sometimes may be misrepresented (Berkowitz, 1971; Wahrman \& Pugh, 1972). Because there have only been two direct replications, one of which by the original author, we believed it was important to revisit the original study. We replicated Schachter's main finding, albeit with a smaller effect size. One intriguing possibility is that we found somewhat weaker reactions to deviates because society may be becoming more tolerant of individuals who hold deviate opinions. We hope that our replication study will inspire other researchers to revisit the deviation-rejection link.},
  number = {3},
  journal = {Social Psychology},
  doi = {10.1027/1864-9335/a000180},
  author = {Wesselmann, Eric D. and Williams, Kipling D. and Pryor, John B. and Eichler, Fredrick A. and Gill, Devin M. and Hogue, John D.},
  month = jan,
  year = {2014},
  pages = {164-169}
}

@article{weston_role_2018,
  title = {The Role of Vigilance in the Relationship between Neuroticism and Health: {{A}} Registered Report},
  volume = {73},
  issn = {0092-6566},
  shorttitle = {The Role of Vigilance in the Relationship between Neuroticism and Health},
  abstract = {The theory of healthy neuroticism, that neuroticism can impact health through both negative and positive pathways, often relies on descriptions of vigilance to illustrate beneficial effects. The current study is among the first to describe the relationship of neuroticism to body vigilance and test the degree to which this relationship impacts health. In an online participant panel (N = 1055), neuroticism was associated with one factor of vigilance: sensation awareness belief. This factor had a suppression effect on the relationship between neuroticism and healthy behavior, such that the effect of neuroticism through vigilance was healthy, whereas the direct effect was unhealthy. We discuss the implications of these findings and caution against using vigilance to explain the heterogeneity in neuroticism-health relationships.},
  journal = {Journal of Research in Personality},
  doi = {10.1016/j.jrp.2017.10.005},
  author = {Weston, Sara J. and Jackson, Joshua J.},
  month = apr,
  year = {2018},
  keywords = {Cross-sectional,Health behaviors,Vigilance,Health,Neuroticism,Preregistered,Self-rated health},
  pages = {27-34}
}

@article{wilkinson_fair_2016,
  title = {The {{FAIR Guiding Principles}} for Scientific Data Management and Stewardship},
  volume = {3},
  copyright = {2016 Nature Publishing Group},
  issn = {2052-4463},
  abstract = {There is an urgent need to improve the infrastructure supporting the reuse of scholarly data. A diverse set of stakeholders\textemdash{}representing academia, industry, funding agencies, and scholarly publishers\textemdash{}have come together to design and jointly endorse a concise and measureable set of principles that we refer to as the FAIR Data Principles. The intent is that these may act as a guideline for those wishing to enhance the reusability of their data holdings. Distinct from peer initiatives that focus on the human scholar, the FAIR Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals. This Comment is the first formal publication of the FAIR Principles, and includes the rationale behind them, and some exemplar implementations in the community.},
  language = {en},
  journal = {Scientific Data},
  doi = {10.1038/sdata.2016.18},
  author = {Wilkinson, Mark D. and Dumontier, Michel and Aalbersberg, IJsbrand Jan and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan-Willem and {da Silva Santos}, Luiz Bonino and Bourne, Philip E. and Bouwman, Jildau and Brookes, Anthony J. and Clark, Tim and Crosas, Merc{\`e} and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo, Chris T. and Finkers, Richard and {Gonzalez-Beltran}, Alejandra and Gray, Alasdair J. G. and Groth, Paul and Goble, Carole and Grethe, Jeffrey S. and Heringa, Jaap and {'t Hoen}, Peter A. C. and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott J. and Martone, Maryann E. and Mons, Albert and Packer, Abel L. and Persson, Bengt and {Rocca-Serra}, Philippe and Roos, Marco and {van Schaik}, Rene and Sansone, Susanna-Assunta and Schultes, Erik and Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz, Morris A. and Thompson, Mark and {van der Lei}, Johan and {van Mulligen}, Erik and Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and Mons, Barend},
  month = mar,
  year = {2016},
  pages = {160018}
}

@book{kitzes_practice_2017,
  title = {The {{Practice}} of {{Reproducible Research}}: {{Case Studies}} and {{Lessons}} from the {{Data}}-{{Intensive Sciences}}},
  isbn = {978-0-520-29475-2},
  shorttitle = {The {{Practice}} of {{Reproducible Research}}},
  abstract = {The Practice of Reproducible Research~presents concrete examples of how researchers in the data-intensive sciences are working to improve the reproducibility of their research projects. In each of the thirty-one case studies in this volume, the author or team describes the workflow that they used to complete a real-world research project. Authors highlight how they utilized particular tools, ideas, and practices to support reproducibility, emphasizing the very practical how, rather than the why or what, of conducting reproducible research. ~ Part 1 provides an accessible introduction to reproducible research, a basic reproducible research project template, and a synthesis of lessons learned from across the thirty-one case studies. Parts 2 and 3 focus on the case studies themselves.~The Practice of Reproducible Research~is an invaluable resource for students and researchers who wish to better understand the practice of data-intensive sciences and learn how to make their own research more reproducible.},
  language = {en},
  publisher = {{Univ of California Press}},
  author = {Kitzes, Justin and Turek, Daniel and Deniz, Fatma},
  month = oct,
  year = {2017},
  keywords = {Science / Earth Sciences / General},
  googlebooks = {NDEyDwAAQBAJ}
}

@article{hardwicke_data_2018,
  title = {Data Availability, Reusability, and Analytic Reproducibility: Evaluating the Impact of a Mandatory Open Data Policy at the Journal {{Cognition}}},
  volume = {5},
  copyright = {\textcopyright{} 2018 The Authors.. Published by the Royal Society under the terms of the Creative Commons Attribution License http://creativecommons.org/licenses/by/4.0/, which permits unrestricted use, provided the original author and source are credited.},
  issn = {2054-5703},
  shorttitle = {Data Availability, Reusability, and Analytic Reproducibility},
  abstract = {Access to data is a critical feature of an efficient, progressive and ultimately self-correcting scientific ecosystem. But the extent to which in-principle benefits of data sharing are realized in practice is unclear. Crucially, it is largely unknown whether published findings can be reproduced by repeating reported analyses upon shared data (`analytic reproducibility'). To investigate this, we conducted an observational evaluation of a mandatory open data policy introduced at the journal Cognition. Interrupted time-series analyses indicated a substantial post-policy increase in data available statements (104/417, 25\% pre-policy to 136/174, 78\% post-policy), although not all data appeared reusable (23/104, 22\% pre-policy to 85/136, 62\%, post-policy). For 35 of the articles determined to have reusable data, we attempted to reproduce 1324 target values. Ultimately, 64 values could not be reproduced within a 10\% margin of error. For 22 articles all target values were reproduced, but 11 of these required author assistance. For 13 articles at least one value could not be reproduced despite author assistance. Importantly, there were no clear indications that original conclusions were seriously impacted. Mandatory open data policies can increase the frequency and quality of data sharing. However, suboptimal data curation, unclear analysis specification and reporting errors can impede analytic reproducibility, undermining the utility of data sharing and the credibility of scientific findings.},
  language = {en},
  number = {8},
  journal = {Open Science},
  doi = {10.1098/rsos.180448},
  author = {Hardwicke, Tom E. and Mathur, Maya B. and MacDonald, Kyle and Nilsonne, Gustav and Banks, George C. and Kidwell, Mallory C. and Mohr, Alicia Hofelich and Clayton, Elizabeth and Yoon, Erica J. and Tessler, Michael Henry and Lenne, Richie L. and Altman, Sara and Long, Bria and Frank, Michael C.},
  month = aug,
  year = {2018},
  pages = {180448},
  note = {00006}
}

@article{morey_peer_2016,
  title = {The {{Peer Reviewers}}' {{Openness Initiative}}: Incentivizing Open Research Practices through Peer Review},
  volume = {3},
  shorttitle = {The {{Peer Reviewers}}' {{Openness Initiative}}},
  number = {1},
  journal = {Royal Society open science},
  author = {Morey, Richard D. and Chambers, Christopher D. and Etchells, Peter J. and Harris, Christine R. and Hoekstra, Rink and Lakens, Dani{\"e}l and Lewandowsky, Stephan and Morey, Candice Coker and Newman, Daniel P. and Sch{\"o}nbrodt, Felix D. and others},
  year = {2016},
  pages = {150547}
}

@article{carlsson_inaugural_2017,
  title = {Inaugural {{Editorial}} of {{Meta}}-{{Psychology}}},
  number = {1},
  journal = {Meta-Psychology},
  doi = {10.15626/MP2017.1001},
  author = {Carlsson, Rickard and Danielsson, Henrik and Heene, Moritz and {Innes-Ker}, Ase and Lakens, Dani{\"e}l and Schimmack, Ulrich and Sch{\"o}nbrodt, Felix D. and {van Asssen}, Marcel and Weinstein, Yana},
  year = {2017},
  pages = {1-3}
}

@article{stodden_empirical_2018,
  title = {An Empirical Analysis of Journal Policy Effectiveness for Computational Reproducibility},
  volume = {115},
  issn = {0027-8424, 1091-6490},
  abstract = {A key component of scientific communication is sufficient information for other researchers in the field to reproduce published findings. For computational and data-enabled research, this has often been interpreted to mean making available the raw data from which results were generated, the computer code that generated the findings, and any additional information needed such as workflows and input parameters. Many journals are revising author guidelines to include data and code availability. This work evaluates the effectiveness of journal policy that requires the data and code necessary for reproducibility be made available postpublication by the authors upon request. We assess the effectiveness of such a policy by (
              i
              ) requesting data and code from authors and (
              ii
              ) attempting replication of the published findings. We chose a random sample of 204 scientific papers published in the journal
              Science
              after the implementation of their policy in February 2011. We found that we were able to obtain artifacts from 44\% of our sample and were able to reproduce the findings for 26\%. We find this policy\textemdash{}author remission of data and code postpublication upon request\textemdash{}an improvement over no policy, but currently insufficient for reproducibility.},
  language = {en},
  number = {11},
  journal = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1708290115},
  author = {Stodden, Victoria and Seiler, Jennifer and Ma, Zhaokun},
  month = mar,
  year = {2018},
  pages = {2584-2589}
}

@article{chang_is_2018,
  title = {Is Economics Research Replicable? Sixty Published Papers from Thirteen Journals Say ``Often Not''},
  volume = {7},
  shorttitle = {Is Economics Research Replicable?},
  journal = {Critical Finance Review},
  author = {Chang, Andrew C. and Li, Phillip},
  year = {2018}
}

@article{arslan_how_2019,
  title = {How to {{Automatically Document Data With}} the Codebook {{Package}} to {{Facilitate Data Reuse}}},
  issn = {2515-2459},
  abstract = {Data documentation in psychology lags behind not only many other disciplines, but also basic standards of usefulness. Psychological scientists often prefer to invest the time and effort that would be necessary to document existing data well in other duties, such as writing and collecting more data. Codebooks therefore tend to be unstandardized and stored in proprietary formats, and they are rarely properly indexed in search engines. This means that rich data sets are sometimes used only once\textemdash{}by their creators\textemdash{}and left to disappear into oblivion. Even if they can find an existing data set, researchers are unlikely to publish analyses based on it if they cannot be confident that they understand it well enough. My codebook package makes it easier to generate rich metadata in human- and machine-readable codebooks. It uses metadata from existing sources and automates some tedious tasks, such as documenting psychological scales and reliabilities, summarizing descriptive statistics, and identifying patterns of missingness. The codebook R package and Web app make it possible to generate a rich codebook in a few minutes and just three clicks. Over time, its use could lead to psychological data becoming findable, accessible, interoperable, and reusable, thereby reducing research waste and benefiting both its users and the scientific community as a whole.},
  language = {en},
  journal = {Advances in Methods and Practices in Psychological Science},
  doi = {10.1177/2515245919838783},
  author = {Arslan, Ruben C.},
  month = may,
  year = {2019},
  pages = {2515245919838783}
}

@article{liu_successes_2019,
  title = {Successes and Struggles with Computational Reproducibility: {{Lessons}} from the {{Fragile Families Challenge}}},
  shorttitle = {Successes and Struggles with Computational Reproducibility},
  abstract = {Reproducibility is fundamental to science, and an important component of reproducibility is computational reproducibility: the ability of a researcher to recreate the results in a published paper using the original author's raw data and code. Although most people agree that computational reproducibility is important, it is still difficult to achieve in practice. In this paper, we describe our approach to enabling computational reproducibility for the 12 papers in this special issue of Socius about the Fragile Families Challenge. Our approach draws on two tools commonly used by professional software engineers but not widely used by academic researchers: software containers (e.g., Docker) and cloud computing (e.g., Amazon Web Services). These tools enabled us to standardize the computing environment around each submission, which will ease computational reproducibility both today and in the future. Drawing on our successes and struggles, we conclude with recommendations to authors and journals.},
  doi = {10.31235/osf.io/g3pdb},
  author = {Liu, David and Salganik, Matthew},
  month = mar,
  year = {2019}
}


